{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import Sampler, BatchSampler\n",
    "from torch.nn.modules.loss import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f5004ec3dfa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "from models import Generator, Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "num_classes = 10\n",
    "batch_size = 256\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./MNIST/', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./MNIST/', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size,         \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient_penalty(discriminator, images, gen_images):\n",
    "        epsilon = torch.FloatTensor(batch_size, 1, 1, 1).uniform_(0., 1.)\n",
    "        epsilon = epsilon.expand(batch_size, images.size(1), images.size(2), images.size(3))\n",
    "        \n",
    "        x_hat = epsilon * images + ((1 - epsilon) * gen_images)\n",
    "        x_hat = Variable(x_hat, requires_grad=True)\n",
    "\n",
    "        prob_x_hat = discriminator(x_hat)\n",
    "        gradients = autograd.grad(outputs=prob_x_hat, inputs=x_hat,\n",
    "                                  grad_outputs=torch.ones_like(prob_x_hat),\n",
    "                                  create_graph=True, retain_graph=True)[0]\n",
    "        \n",
    "        dual_sobolev_gradients = sobolev_filter(gradients, c=SOBOLEV_C, s=-SOBOLEV_S)\n",
    "        gradients_stable_norm = stable_norm(dual_sobolev_gradients, ord=DUAL_EXPONENT)\n",
    "        \n",
    "        lambda_ = stable_norm(sobolev_filter(images, c=SOBOLEV_C, s=SOBOLEV_S),\n",
    "                              ord=EXPONENT).mean()\n",
    "        gamma_ = stable_norm(sobolev_filter(images, c=SOBOLEV_C, s=-SOBOLEV_S),\n",
    "                              ord=DUAL_EXPONENT).mean()\n",
    "        \n",
    "        prob_images = discriminator(images)\n",
    "        \n",
    "        grad_penalty = ((gradients_stable_norm / gamma_ - 1) ** 2).mean() * lambda_ +\\\n",
    "                       1e-5 * (prob_images ** 2).mean()\n",
    "                        \n",
    "        return grad_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0f6993a7c004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Generator' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "max_iters = 100000\n",
    "num_disc_iters = 5\n",
    "noise_size = 128\n",
    "\n",
    "def generate_batches(train_loader):\n",
    "    while True:\n",
    "        for batch_num, (x_batch_base, _) in zip(trange(len(train_loader)), train_loader):\n",
    "            yield batch_num, x_batch_base.float()\n",
    "            \n",
    "#Models\n",
    "generator = Generator(noise_size)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "lr = 1e-4\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "# Data-Generator\n",
    "data = generate_batches(train_loader)\n",
    "\n",
    "for global_iter in range(max_iters):\n",
    "    \n",
    "    for d_iter in range(num_disc_iters):\n",
    "        batch_num, images = data.__next__()\n",
    "        z = torch.randn((batch_size, noise_size, 1, 1))\n",
    "        images, z = Variable(images), Variable(z)\n",
    "        \n",
    "        discriminator.zero_grad()\n",
    "        gen_images = generator(z)\n",
    "        fake_loss = discriminator(gen_images).mean()\n",
    "        fake_loss.backward()\n",
    "        \n",
    "        real_loss = discriminator(images).mean()\n",
    "        real_loss.backward(mone)\n",
    "\n",
    "        gradient_penalty = calculate_gradient_penalty(discriminator, images.data, gen_images.data)\n",
    "        gradient_penalty.backward()\n",
    "        \n",
    "        wasserstein_loss = (fake_loss - real_loss) / gamma\n",
    "\n",
    "        g_loss = (gen_images).mean() / gamma\n",
    "        d_loss = - wasserstein_loss + gradient_penalty\n",
    "        \n",
    "        optimizer_D.step()\n",
    "    \n",
    "    generator.zero_grad()\n",
    "            \n",
    "    z = Variable(torch.randn(batch_size, noise_size, 1, 1))\n",
    "    gen_images = generator(z)\n",
    "    g_loss = discriminator(gen_images).mean()\n",
    "    g_loss.backward(mone)\n",
    "            \n",
    "    optimizer_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
