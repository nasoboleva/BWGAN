{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.misc import imsave\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import Sampler, BatchSampler\n",
    "from torch.nn.modules.loss import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Generator, Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./MNIST/', \n",
    "                            train=True, \n",
    "                            transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                         transforms.Lambda(lambda x: (x - 0.5) * 2)]),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./MNIST/', \n",
    "                           train=False, \n",
    "                           transform= transforms.Compose([transforms.ToTensor(),\n",
    "                                                         transforms.Lambda(lambda x: (x - 0.5) * 2)]),\n",
    "                          )\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False, drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=test_batch_size,         \n",
    "                                          shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autograd\n",
    "from norms import sobolev_norm, lp_norm\n",
    "\n",
    "l2_norm = 2\n",
    "dual_norm = 1 / (1 - 1 / l2_norm) if l2_norm != 1 else np.inf\n",
    "\n",
    "c_for_sobolev = 5.0\n",
    "s_for_sobolev = 0\n",
    "\n",
    "def calculate_gradient_penalty(discriminator, images, gen_images):\n",
    "        epsilon = torch.FloatTensor(images.size(0), 1, 1, 1).uniform_(0., 1.).cuda()\n",
    "        epsilon = epsilon.expand(images.size(0), images.size(1), images.size(2), images.size(3))\n",
    "        \n",
    "        x_hat = epsilon * images + ((1 - epsilon) * gen_images)\n",
    "        x_hat = Variable(x_hat, requires_grad=True)\n",
    "\n",
    "        prob_x_hat = discriminator(x_hat)\n",
    "        gradients = autograd.grad(outputs=prob_x_hat, inputs=x_hat,\n",
    "                                  grad_outputs=torch.ones_like(prob_x_hat).cuda(),\n",
    "                                  create_graph=True, retain_graph=True)[0]\n",
    "        \n",
    "        dual_sobolev_gradients = sobolev_norm(gradients, s=-s_for_sobolev, c=c_for_sobolev)\n",
    "        gradients_norm = lp_norm(dual_sobolev_gradients, p=dual_norm)\n",
    "        \n",
    "        lambda_ = lp_norm(sobolev_norm(images,  s=s_for_sobolev, c=c_for_sobolev),\n",
    "                              p=l2_norm).mean()\n",
    "        gamma_ = lp_norm(sobolev_norm(images, s=-s_for_sobolev, c=c_for_sobolev),\n",
    "                              p=dual_norm).mean()\n",
    "        \n",
    "        prob_images = discriminator(images)\n",
    "        \n",
    "        grad_penalty = ((gradients_norm.float().cuda() / gamma_.float().cuda()  - 1) ** 2).mean() * lambda_.float().cuda()  +\\\n",
    "                       1e-5 * (prob_images.float().cuda()  ** 2).mean()            \n",
    "        return grad_penalty, gamma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(generator, discriminator, test_batch_generator):\n",
    "    generator.eval()\n",
    "    batch_num, images = test_batch_generator.__next__()\n",
    "    z = torch.randn((batch_size, noise_size)).cuda()\n",
    "    images, z = Variable(images.cuda()), Variable(z)\n",
    "    \n",
    "    gen_images = generator(z)\n",
    "    fake_loss = discriminator(gen_images).mean() \n",
    "    real_loss = discriminator(images).mean()\n",
    "\n",
    "    gradient_penalty, gamma = calculate_gradient_penalty(discriminator, images.data, gen_images.data)\n",
    "        \n",
    "    wasserstein_loss = (fake_loss - real_loss) / gamma\n",
    "\n",
    "    g_loss = (gen_images).mean() / gamma\n",
    "    d_loss = - wasserstein_loss + gradient_penalty\n",
    "    \n",
    "    return g_loss.item(), d_loss.item(), gradient_penalty.item()\n",
    "\n",
    "  \n",
    "def plot_history(train_history, val_history, title='loss'):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(train_history['g_loss'], label='train_generator_loss', zorder=1)\n",
    "    plt.plot(train_history['d_loss'], label='train_discrimintor_loss', zorder=1)\n",
    "    points_g = np.array(val_history['g_loss'])\n",
    "    points_d = np.array(val_history['d_loss'])\n",
    "    plt.scatter(points_g[:, 0], points_g[:, 1], marker='+', s=180, c='orange', label='val_generator', zorder=2)\n",
    "    plt.scatter(points_d[:, 0], points_d[:, 1], marker='+', s=180, c='red', label='val_discriminator', zorder=2)\n",
    "    plt.xlabel('train steps')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Gradient Penalty')\n",
    "    plt.plot(train_history['grad_pen'], label='gradient_penalty', zorder=1)\n",
    "    points_d = np.array(val_history['grad_pen'])\n",
    "    plt.scatter(points_g[:, 0], points_g[:, 1], marker='+', s=180, c='orange', label='val_gradient_penalty', zorder=2)\n",
    "    plt.xlabel('train steps')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from torch.autograd import Variable\n",
    "\n",
    "max_iters = 100000\n",
    "val_freq = 10\n",
    "num_disc_iters = 5\n",
    "noise_size = 128\n",
    "channels = 1\n",
    "img_size = 28\n",
    "\n",
    "def generate_batches(train_loader):\n",
    "    while True:\n",
    "        for batch_num, (x_batch_base, _) in zip(trange(len(train_loader)), train_loader):\n",
    "            yield batch_num, x_batch_base.float()\n",
    "            \n",
    "#Models\n",
    "generator = Generator(noise_size, channels, img_size).cuda()\n",
    "discriminator = Discriminator(noise_size, channels).cuda()\n",
    "\n",
    "lr = 1e-4\n",
    "beta1 = 0.\n",
    "beta2 = 0.9\n",
    "    \n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "# Data-Generator\n",
    "data = generate_batches(train_loader)\n",
    "test_data = generate_batches(test_loader)\n",
    "\n",
    "train_log = {'g_loss': [], 'd_loss' : [], 'grad_pen' : []}\n",
    "val_log = {'g_loss': [], 'd_loss' : [], 'grad_pen' : []}\n",
    "for global_iter in range(max_iters):\n",
    "    generator.eval()\n",
    "    for d_iter in range(num_disc_iters):\n",
    "        batch_num, images = data.__next__()\n",
    "        z = torch.randn((batch_size, noise_size)).cuda()\n",
    "        images, z = Variable(images.cuda()), Variable(z)\n",
    "        \n",
    "        discriminator.zero_grad()\n",
    "        gen_images = generator(z)\n",
    "        fake_loss = discriminator(gen_images).mean()\n",
    "        fake_loss.backward()\n",
    "        \n",
    "        real_loss = discriminator(images).mean()\n",
    "        real_loss.backward(torch.FloatTensor([-1]).cuda())\n",
    "\n",
    "        gradient_penalty, gamma = calculate_gradient_penalty(discriminator, images.data, gen_images.data)\n",
    "        gradient_penalty.backward()\n",
    "        \n",
    "        wasserstein_loss = (fake_loss - real_loss) / gamma\n",
    "\n",
    "        g_loss = (gen_images).mean() / gamma\n",
    "        d_loss = - wasserstein_loss + gradient_penalty\n",
    "        \n",
    "        optimizer_D.step()\n",
    "        \n",
    "        \n",
    "        train_log['g_loss'] += [g_loss.item()]\n",
    "        train_log['d_loss'] += [d_loss.item()]\n",
    "        train_log['grad_pen'] += [gradient_penalty.item()]\n",
    "            #clear_output(True)\n",
    "            #print (\"\\n[Global iter %d/%d] [Discr iter %d/%d] [D loss: %f] [G adv: %f] [Gradient Penalty: %f]\" \n",
    "            #       % (global_iter, max_iters, d_iter, num_disc_iters, \n",
    "            #          d_loss.item(), g_loss.item(), gradient_penalty.item()))\n",
    "    \n",
    "    generator.train()\n",
    "    generator.zero_grad()\n",
    "            \n",
    "    z = Variable(torch.randn(batch_size, noise_size)).cuda()\n",
    "    gen_images = generator(z)\n",
    "    g_loss = discriminator(gen_images).mean()\n",
    "    g_loss.backward(torch.FloatTensor([-1]).cuda())\n",
    "            \n",
    "    optimizer_G.step()\n",
    "   \n",
    "    if global_iter % val_freq == val_freq - 1:\n",
    "        g_loss_log, d_loss_log, gradient_penalty_log = test(generator, discriminator, test_data)\n",
    "        val_log['g_loss'] += [(global_iter * (num_disc_iters), np.mean(g_loss_log))]\n",
    "        val_log['d_loss'] += [(global_iter * (num_disc_iters), np.mean(d_loss_log))]\n",
    "        val_log['grad_pen'] += [(global_iter * (num_disc_iters), np.mean(gradient_penalty_log))]\n",
    "        clear_output()\n",
    "        plot_history(train_log, val_log)\n",
    "        \n",
    "        numpy_images = gen_images.cpu().detach().numpy()\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        \n",
    "        for i in range(5):\n",
    "            plt.subplot(1, 5, i + 1)\n",
    "            plt.imshow(numpy_images[i].reshape((28, 28)), cmap=\"gray\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_big_image(images_64):\n",
    "    res = []\n",
    "    for i in range(8):\n",
    "        res.append([x for x in images[i * 8:(i + 1) * 8]])\n",
    "    return np.block(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_gen_images = gen_images.cpu().detach().numpy()\n",
    "grid_images = get_big_image(numpy_gen_images[:64]).squeeze()\n",
    "imsave(\"fake.png\", grid_images)\n",
    "plt.imshow(grid_images, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_images = images.cpu().detach().numpy()\n",
    "grid_images = get_big_image(numpy_images[:64]).squeeze()\n",
    "imsave(\"real.png\", grid_images)\n",
    "plt.imshow(grid_images, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), \"generator.weights\")\n",
    "torch.save(discriminator.state_dict(), \"discriminator.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To resore weights\n",
    "# generator = Generator(noise_size, channels, img_size).cuda()\n",
    "# generator.load_state_dict(torch.load(\"generator.weights\"))\n",
    "# generator.eval()\n",
    "# discriminator = Discriminator(noise_size, channels).cuda()\n",
    "# discriminator.load_state_dict(torch.load(\"discriminator.weights\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
